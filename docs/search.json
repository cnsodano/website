[
  {
    "objectID": "projects/index.html",
    "href": "projects/index.html",
    "title": "Recent Projects",
    "section": "",
    "text": "Recent Projects\n\nAutomating Reproducibility Checks for Journal Submissions\nInspired by conversations at the IGDORE Reproducibilitea journal club, this project aims to automate the simpler aspects of the reproducibility check process for code submitted alongside a journal article. I developed a script that parses a codebase and notifies users of the following barriers to reproducibility:\n\nFiles that are referenced in the code but are missing from the codebase\nAbsolute paths to files that are machine-specific (e.g. “C:/Users/Desktop/Data/data.csv”)\n\nPaths which are not resolvable (the locations don’t exist in the database) are caught\nPaths which are resolvable (the locations do exist) are replaced with relative paths (e.g. “../Data/data.csv”)\n\n\nThis project is still in early development. Try scanning a single file below (requires javascript to be enabled):\n\n\nDetecting Post-Retraction Citation Awareness\nThis project aims to discern whether an article that cites a retracted article, and which is published after the retraction notice was released, is doing so knowingly (and acknowledging) the retraction or not. I began the project with the intention of providing data to inform the discussion in metascience about whether and how damaging postretraction citations are. Inspired by Hsiao & Schneider 2022\n\n\nHijacked Journal Detection Toolbox\nInspired by Anna Abalkina’s reporting on the hijacking of the Russian Law Journal, I used Crossref’s API to investigate how these paper mill articles were able to receive valid DOIs, and discovered a new method fraudsters are using to spoof the identity of journals they are hijacking. Rather than just buying the web-domain or typosquatting a similar web domain, they are able to register DOIs under the same journal title in 3rd party DOI provider databases like Crossref. In the process of investigating these hijackings, I found myself using a set of API calls and scripts often, and I am developing them into a browser extension to help other sleuths quickly check for signs of a journal hijacking.\n\n\nDiscovering Unreasonable Citation Stacking\nI’m developing an algorithm to scan the scientific literature and identify authors that have an unusually high number of papers with signs of citation stacking (the presence of many citations to a single author (or group of authors) that are not the result of honest scientific work but rather the result of gaming citation metrics). I’m planning to turn this into a package that is inter-operable with many types of data sources (Scopus, Crossref, OpenAlex) for others to use. Inspired by El País’s reporting on a case of extreme self-citation stacking among an esteemed computer scientist.\n\n\nPredicting Paper Acceptance Status from Peer Reviews\nI used Computer Science conference paper peer reviews to train a ML model to predict whether the paper was accepted or rejected. If done well, algorithms like these can lighten the load of editors’ jobs by helping them synthesize or make decisions on acceptance/rejection altogether (though much improvement must be made before that responsibility can be safely passed). First attempts had poorer performance than I had hoped for, so I am currently revamping with a more sophisticated feature representation that will hopefully capture more nuance.\n\n\n\n\nOlder Projects\n\nDetecting and Denoising EEG Artifacts\nReplicating the results of Li et al. 2023. Created semi-simulated noisy EEG data using an open dataset of common EEG artifacts to train a segmentation neural network to detect where artifacts occur. Although an adjustment needed to be made to match the codebase with the presented methods in the paper, the accuracy of the artifact detection network largely replicated. Approaches like these are promising, as artifact removal is both a highly time-consuming and bias-injecting step in the standard preprocessing pipeline of EEG experiments, however I seriously question the generalizability of this group’s model due to major flaws in the assumptions behind the ‘clean’ EEG dataset used in training.\n\n\nAutomatically Detecting Sleep Spindles on EEG\nReplicating the best performing algorithm from Lacourse et al. 2018 and comparing to a replicated version of Lustenberger et al. 2016. Adding fuzzy logic to the sliding window reduce false negatives on by-event analysis due to singular sample outliers.\n\n\nCognitive Task Anti-Cheating\nSpinoff analysis that came from working on a working memory cognitive task experiment where participants were instructed to attend to the center of their visual field and were prompted to recall stimuli according to, among other things, which visual hemifield the stimuli were presented in. Explored the feasibility of detecting a bias in performance on one side that would suggest participants are not attending to the center of their visual field, but rather ‘cheating’ to one side to improve their odds by minimizing the amount of information they must hold in their working memory."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Christian N Sodano",
    "section": "",
    "text": "Bluesky\n  \n  \n    \n     Github\n  \n\n  \n  \n\n\nI’m a researcher currently working in the emerging field of forensic scientometrics1 and developing tools to detect and prevent breaches of research integrity including questionable research practices, statistical and methodological errors, and fraud.\nMy current projects in this area focus on detecting so-called paper-mills (businesses conducting for-profit research fraud at large-scale) and journal hijackings, as well as authors who game citations for personal benefit and papers that are unknowingly citing discredited research.\n\n\n\nI use openly accessible APIs to extract and analyze scholarly metadata from a variety of sources (Scopus, OpenAlex, Crossref) and analyze trends. I also perform natural language processing on full-text data of open access articles (e.g. PubMed Open Access Corpus) and peer reviews. I have implemented anomaly detectors using various techniques, including data-driven approaches, time-frequency techniques, and neural networks."
  },
  {
    "objectID": "index.html#summary",
    "href": "index.html#summary",
    "title": "Christian N Sodano",
    "section": "",
    "text": "I’m a researcher currently working in the emerging field of forensic scientometrics1 and developing tools to detect and prevent breaches of research integrity including questionable research practices, statistical and methodological errors, and fraud.\nMy current projects in this area focus on detecting so-called paper-mills (businesses conducting for-profit research fraud at large-scale) and journal hijackings, as well as authors who game citations for personal benefit and papers that are unknowingly citing discredited research."
  },
  {
    "objectID": "index.html#techniques",
    "href": "index.html#techniques",
    "title": "Christian N Sodano",
    "section": "",
    "text": "I use openly accessible APIs to extract and analyze scholarly metadata from a variety of sources (Scopus, OpenAlex, Crossref) and analyze trends. I also perform natural language processing on full-text data of open access articles (e.g. PubMed Open Access Corpus) and peer reviews. I have implemented anomaly detectors using various techniques, including data-driven approaches, time-frequency techniques, and neural networks."
  },
  {
    "objectID": "index.html#footnotes",
    "href": "index.html#footnotes",
    "title": "Christian N Sodano",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nMore on Forensic Scientometrics, and the allied forensic metascience↩︎"
  }
]